{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, AutoTokenizer, AutoModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep='\\t', quoting=3)\n",
    "train_xnli = pd.read_csv(\"multinli.train.ko.tsv\", sep='\\t', quoting=3)\n",
    "val_data = pd.read_csv(\"xnli.dev.ko.tsv\", sep='\\t', quoting=3)\n",
    "test_data = pd.read_csv(\"xnli.test.ko.tsv\", sep='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.</td>\n",
       "      <td>제품과 지리학은 크림 스키밍을 작동시키는 것이다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스...</td>\n",
       "      <td>사람들이 기억하면 다음 수준으로 물건을 잃는다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.</td>\n",
       "      <td>우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>어떻게 아세요? 이 모든 것이 다시 그들의 정보다.</td>\n",
       "      <td>이 정보는 그들의 것이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그래, 만약 네가 테니스화 몇 개를 사러 간다면, 나는 왜 그들이 100달러대에서 ...</td>\n",
       "      <td>테니스화의 가격은 다양하다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0         개념적으로 크림 스키밍은 제품과 지리라는 두 가지 기본 차원을 가지고 있다.   \n",
       "1  시즌 중에 알고 있는 거 알아? 네 레벨에서 다음 레벨로 잃어버리는 거야 브레이브스...   \n",
       "2                  우리 번호 중 하나가 당신의 지시를 세밀하게 수행할 것이다.   \n",
       "3                       어떻게 아세요? 이 모든 것이 다시 그들의 정보다.   \n",
       "4  그래, 만약 네가 테니스화 몇 개를 사러 간다면, 나는 왜 그들이 100달러대에서 ...   \n",
       "\n",
       "                              sentence2  gold_label  \n",
       "0           제품과 지리학은 크림 스키밍을 작동시키는 것이다.     neutral  \n",
       "1            사람들이 기억하면 다음 수준으로 물건을 잃는다.  entailment  \n",
       "2  우리 팀의 일원이 당신의 명령을 엄청나게 정확하게 실행할 것이다.  entailment  \n",
       "3                        이 정보는 그들의 것이다.  entailment  \n",
       "4                       테니스화의 가격은 다양하다.     neutral  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xnli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결합 후 섞기\n",
    "train_data = train_snli.append(train_xnli)\n",
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158515</th>\n",
       "      <td>성과가 높은 조직은 또한 조직의 낮은 수준으로 권한을 위임함으로써 직원을 참여시키고...</td>\n",
       "      <td>실적이 낮은 조직들도 직원들을 참여시키고 참여시키기 위해 노력한다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217758</th>\n",
       "      <td>하나는 다른 하나가 빼앗은 것을 복원한다.\"</td>\n",
       "      <td>하나는 다른 하나를 대신한다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453696</th>\n",
       "      <td>헤드기어와 복싱 글러브를 착용한 남성이 복싱 글러브만 착용한 여성과 함께 훈련하고 있다.</td>\n",
       "      <td>한 사람이 장갑을 낀 여자와 훈련을 하고 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124475</th>\n",
       "      <td>아름다운 날에 한 남자가 골프를 치고 있다.</td>\n",
       "      <td>테니스 치는 여자.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115197</th>\n",
       "      <td>셔츠, 바지, 모자를 매치한 세 남자와 가운을 입은 한 남자가 방에 있다.</td>\n",
       "      <td>방에 있는 남자들</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "158515  성과가 높은 조직은 또한 조직의 낮은 수준으로 권한을 위임함으로써 직원을 참여시키고...   \n",
       "217758                           하나는 다른 하나가 빼앗은 것을 복원한다.\"   \n",
       "453696  헤드기어와 복싱 글러브를 착용한 남성이 복싱 글러브만 착용한 여성과 함께 훈련하고 있다.   \n",
       "124475                           아름다운 날에 한 남자가 골프를 치고 있다.   \n",
       "115197          셔츠, 바지, 모자를 매치한 세 남자와 가운을 입은 한 남자가 방에 있다.   \n",
       "\n",
       "                                   sentence2     gold_label  \n",
       "158515  실적이 낮은 조직들도 직원들을 참여시키고 참여시키기 위해 노력한다        neutral  \n",
       "217758                      하나는 다른 하나를 대신한다.        neutral  \n",
       "453696            한 사람이 장갑을 낀 여자와 훈련을 하고 있다.     entailment  \n",
       "124475                            테니스 치는 여자.  contradiction  \n",
       "115197                             방에 있는 남자들     entailment  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_and_duplicates(df) :\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측값 및 중복 샘플 제거\n",
    "train_data = drop_na_and_duplicates(train_data)\n",
    "val_data = drop_na_and_duplicates(val_data)\n",
    "test_data = drop_na_and_duplicates(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>성과가 높은 조직은 또한 조직의 낮은 수준으로 권한을 위임함으로써 직원을 참여시키고...</td>\n",
       "      <td>실적이 낮은 조직들도 직원들을 참여시키고 참여시키기 위해 노력한다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>하나는 다른 하나가 빼앗은 것을 복원한다.\"</td>\n",
       "      <td>하나는 다른 하나를 대신한다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>헤드기어와 복싱 글러브를 착용한 남성이 복싱 글러브만 착용한 여성과 함께 훈련하고 있다.</td>\n",
       "      <td>한 사람이 장갑을 낀 여자와 훈련을 하고 있다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아름다운 날에 한 남자가 골프를 치고 있다.</td>\n",
       "      <td>테니스 치는 여자.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>셔츠, 바지, 모자를 매치한 세 남자와 가운을 입은 한 남자가 방에 있다.</td>\n",
       "      <td>방에 있는 남자들</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941809</th>\n",
       "      <td>중국 시장에서 카메라를 보고 있는 관광객 두어 명.</td>\n",
       "      <td>관광객들이 호텔에 있다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941810</th>\n",
       "      <td>판사가 배심원을 상대로 한 판결에 대한 흥미로운 어 포인트 나는 그것과 함께 올 수...</td>\n",
       "      <td>판사의 판결에 대한 흥미로운 점 나는 그것과 함께 올 학대를 받아들일지 모르겠다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941811</th>\n",
       "      <td>enclos paroissial(파리쉬 클로즈)은 16세기에서 18세기 동안 브리트...</td>\n",
       "      <td>가까운 교구는 현대적인 도시 환경이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941812</th>\n",
       "      <td>알고 보니, 적어도 우리만큼 따뜻하지는 않지만, 그들을 우리의 직장과 학교에 환영하...</td>\n",
       "      <td>우리가 전에 그랬던 것처럼 그들을 환영해도 괜찮다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941813</th>\n",
       "      <td>그것은 처벌이 지금 범죄에 맞지 않는다.</td>\n",
       "      <td>요즘의 처벌은 그 범죄에 좀처럼 적합하지 않다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>941814 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "0       성과가 높은 조직은 또한 조직의 낮은 수준으로 권한을 위임함으로써 직원을 참여시키고...   \n",
       "1                                하나는 다른 하나가 빼앗은 것을 복원한다.\"   \n",
       "2       헤드기어와 복싱 글러브를 착용한 남성이 복싱 글러브만 착용한 여성과 함께 훈련하고 있다.   \n",
       "3                                아름다운 날에 한 남자가 골프를 치고 있다.   \n",
       "4               셔츠, 바지, 모자를 매치한 세 남자와 가운을 입은 한 남자가 방에 있다.   \n",
       "...                                                   ...   \n",
       "941809                       중국 시장에서 카메라를 보고 있는 관광객 두어 명.   \n",
       "941810  판사가 배심원을 상대로 한 판결에 대한 흥미로운 어 포인트 나는 그것과 함께 올 수...   \n",
       "941811  enclos paroissial(파리쉬 클로즈)은 16세기에서 18세기 동안 브리트...   \n",
       "941812  알고 보니, 적어도 우리만큼 따뜻하지는 않지만, 그들을 우리의 직장과 학교에 환영하...   \n",
       "941813                             그것은 처벌이 지금 범죄에 맞지 않는다.   \n",
       "\n",
       "                                            sentence2     gold_label  \n",
       "0                실적이 낮은 조직들도 직원들을 참여시키고 참여시키기 위해 노력한다        neutral  \n",
       "1                                    하나는 다른 하나를 대신한다.        neutral  \n",
       "2                          한 사람이 장갑을 낀 여자와 훈련을 하고 있다.     entailment  \n",
       "3                                          테니스 치는 여자.  contradiction  \n",
       "4                                           방에 있는 남자들     entailment  \n",
       "...                                               ...            ...  \n",
       "941809                                  관광객들이 호텔에 있다.  contradiction  \n",
       "941810  판사의 판결에 대한 흥미로운 점 나는 그것과 함께 올 학대를 받아들일지 모르겠다.     entailment  \n",
       "941811                          가까운 교구는 현대적인 도시 환경이다.  contradiction  \n",
       "941812                   우리가 전에 그랬던 것처럼 그들을 환영해도 괜찮다.  contradiction  \n",
       "941813                     요즘의 처벌은 그 범죄에 좀처럼 적합하지 않다.     entailment  \n",
       "\n",
       "[941814 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kobert토크나이저 pre train 버트모델 불러와 사용\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "import gluonnlp as nlp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class koBERTNLI(nn.Module):\n",
    "    def __init__(self, kobert, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.kobert = kobert\n",
    "        self.classifier = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.kobert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        outputs = self.classifier(outputs[1])\n",
    "        outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_num_dict = {\n",
    "    'contradiction':0,\n",
    "    'entailment':1,\n",
    "    'neutral':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#두 문장을 입력받아서 하나의 input으로 바꿔줌\n",
    "class NLIDataset(Dataset):\n",
    "  def __init__(self, dataset, tokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.dataset = dataset\n",
    "    sentence1 = self.dataset['sentence1'].tolist()\n",
    "    sentence2 = self.dataset['sentence2'].tolist()\n",
    "    inputs = tokenizer(sentence1, sentence2, padding=True, truncation=True)\n",
    "    self.input_ids = inputs['input_ids']\n",
    "    self.attention_mask = inputs['attention_mask']\n",
    "    self.token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "    self.labels = self.dataset['gold_label'].tolist()\n",
    "    self.labels = [label_to_num_dict[l] for l in self.labels]\n",
    "    self.labels = F.one_hot(torch.tensor(self.labels), num_classes=3)\n",
    "    self.labels = self.labels.tolist()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return (self.input_ids[idx], self.attention_mask[idx], self.token_type_ids[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    attention_mask = [item[1] for item in batch]\n",
    "    token_type_ids = [item[2] for item in batch]\n",
    "    labels = [item[3] for item in batch]\n",
    "    return torch.LongTensor(input_ids), torch.LongTensor(attention_mask), torch.LongTensor(token_type_ids), torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NLIDataset(train_data, tokenizer)\n",
    "valid_ds = NLIDataset(val_data, tokenizer)\n",
    "test_ds = NLIDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = koBERTNLI(kobert=bertmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼파라미터\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "log_interval = 1000\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(total_steps * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_step,\n",
    "                                            num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0e85e2716442d09ae7ffaea53a3a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/941814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.53 GiB already allocated; 0 bytes free; 4.78 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mf:\\kornli\\1.ipynb 셀 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(out, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32mf:\\kornli\\1.ipynb 셀 22\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkobert(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(outputs[\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/kornli/1.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mf:\\Projects\\kornli\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:325\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    322\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    324\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    328\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.53 GiB already allocated; 0 bytes free; 4.78 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#실제 학습 진행 부분\n",
    "best_loss = float('inf')\n",
    "print(device)\n",
    "for e in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, attention_mask, token_type_ids, labels) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        loss = loss_fn(out, labels)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(f\"epoch {e+1} batch id {batch_id+1} train loss {(epoch_loss/(batch_id+1)):.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "    for batch_id, (input_ids, attention_mask, token_type_ids, labels) in enumerate(tqdm_notebook(valid_dataloader)):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        loss = loss_fn(out, labels)\n",
    "        valid_loss += loss.item()\n",
    "    valid_loss = valid_loss/len(valid_dataloader)\n",
    "    print(f\"epoch {e+1} batch id {batch_id+1} valid loss {(valid_loss):.3f}\")\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_epoch = e+1\n",
    "\n",
    "        save_path = \"./model.pt\"\n",
    "        torch.save({\n",
    "            \"epoch\" : best_epoch,\n",
    "            \"model_state_dict\" : model.state_dict(),\n",
    "            \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "            \"loss\" : best_loss,\n",
    "        }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint state. epoch is 1, loss is 1.0987966125592208\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    checkpoint = torch.load(\"./model.pt\")\n",
    "    epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Load checkpoint state. epoch is {epoch}, loss is {loss}\")\n",
    "except:\n",
    "    print(\"No checkpoint state exits\")\n",
    "    epoch = 0\n",
    "    loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input two sentence\n",
      "sent1: 아 빨리 자고싶다\n",
      "sent1: 어우 졸려 이제 일해야지\n"
     ]
    }
   ],
   "source": [
    "print(\"input two sentence\")\n",
    "sent1 = input(\"sentence 1: \")\n",
    "sent2 = input(\"sentence 2: \")\n",
    "\n",
    "print(f\"sent1: {sent1}\")\n",
    "print(f\"sent1: {sent2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#확인용\n",
    "class NLIDataset2(Dataset):\n",
    "  def __init__(self, sent1, sent2, tokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    sentence1 = []\n",
    "    sentence2 = []\n",
    "\n",
    "    sentence1.append(sent1)\n",
    "    sentence2.append(sent2)\n",
    "\n",
    "    inputs = tokenizer(sentence1, sentence2, padding=True, truncation=True)\n",
    "    self.input_ids = inputs['input_ids']\n",
    "    self.attention_mask = inputs['attention_mask']\n",
    "    self.token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return (self.input_ids[idx], self.attention_mask[idx], self.token_type_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn2(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    attention_mask = [item[1] for item in batch]\n",
    "    token_type_ids = [item[2] for item in batch]\n",
    "    return torch.LongTensor(input_ids), torch.LongTensor(attention_mask), torch.LongTensor(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "inference_ds = NLIDataset2(sent1, sent2, tokenizer)\n",
    "batch_size = 1\n",
    "infer_dataloader = torch.utils.data.DataLoader(inference_ds, batch_size=batch_size, collate_fn=collate_fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 빨리 자고싶다\n",
      "어우 졸려 이제 일해야지\n",
      "contradiction:\t 0.33877\n",
      "entailment:\t 0.35578\n",
      "neutral:\t 0.30546\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch_id, (input_ids, attention_mask, token_type_ids) in enumerate(infer_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    outputs = out.squeeze().tolist()\n",
    "\n",
    "print(sent1)\n",
    "print(sent2)\n",
    "print(\"contradiction:\\t {:.5f}\".format(outputs[0]))\n",
    "print(\"entailment:\\t {:.5f}\".format(outputs[1]))\n",
    "print(\"neutral:\\t {:.5f}\".format(outputs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [30/Oct/2023 00:11:59] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 반갑습니다 \n",
      "아 오늘 또 만났네\n",
      "contradiction:\t 0.33877\n",
      "entailment:\t 0.35578\n",
      "neutral:\t 0.30546\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/api', methods=['POST'])\n",
    "def post_json():\n",
    "    data = request.get_json()\n",
    "    sent1 = data['answer']\n",
    "    sent2 = \"아 오늘 또 만났네\"\n",
    "    \n",
    "    inference_ds = NLIDataset2(sent1, sent2, tokenizer)\n",
    "    batch_size = 1\n",
    "    infer_dataloader = torch.utils.data.DataLoader(inference_ds, batch_size=batch_size, collate_fn=collate_fn2)\n",
    "\n",
    "    model.eval()\n",
    "    for batch_id, (input_ids, attention_mask, token_type_ids) in enumerate(infer_dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    outputs = out.squeeze().tolist()\n",
    "\n",
    "    print(sent1)\n",
    "    print(sent2)\n",
    "    print(\"contradiction:\\t {:.5f}\".format(outputs[0]))\n",
    "    print(\"entailment:\\t {:.5f}\".format(outputs[1]))\n",
    "    print(\"neutral:\\t {:.5f}\".format(outputs[2]))\n",
    "    response_data = {\n",
    "        'message' : outputs[0]\n",
    "    }\n",
    "    return jsonify(response_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kornli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
